--- 
title: "Introduction to Nextflow"
author1: "Akshai Parakkal Sreenivasan"
author2: "Payam Emami"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
description: |
 This is a short tutorial about Nextflow. 
link-citations: yes
github-repo: rstudio/bookdown-demo
output:
  bookdown::html_document2:
    includes:
      in_header: header.html
  bookdown::gitbook:
    includes:
      in_header: header.html
---

```{r setup, include=FALSE}
# Add a common class name for every chunks
knitr::opts_chunk$set(
  echo = TRUE)
```
```{r htmlTemp3, echo=FALSE, eval=TRUE}
# install.packages("readr") # Enable this the first time to install the package
codejs <- readr::read_lines("js/codefolding.js")
collapsejs <- readr::read_lines("js/collapse.js")
transitionjs <- readr::read_lines("js/transition.js")

htmlhead <- 
  paste('
<script>',
paste(transitionjs, collapse = "\n"),
'</script>
<script>',
paste(collapsejs, collapse = "\n"),
'</script>
<script>',
paste(codejs, collapse = "\n"),
'</script>
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
.row { display: flex; }
.collapse { display: none; }
.in { display:block }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>
', sep = "\n")

readr::write_lines(htmlhead, path = "header.html")
```

# Introduction

Welcome to this short tutorial on Nextflow. We are going to walk through some of the functionalities of this powerful workflow engine. More specifically we are going to do some hands-on work on processes, channels and operators. We are also going to look into how to configure Nextflow to run on different platforms. But before moving forward, we need to set up the environment for using Nextflow. 

## Setting up the environment

We are going to use UPPMAX for doing most of the hands-on work. 

First ssh to UPPMAX

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
ssh -AX youusername@rackham.uppmax.uu.se
```

Start an interactive uppmax session

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
interactive -A uppmax2024-2-11 -M snowy -t 02:00:00
```

Make a directory in your user space (only if you have not done before)

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mkdir -p /crex/proj/uppmax2024-2-11/nobackup/$USER/nextflow_lab1
```

Navigate to the folder you have created

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
cd /crex/proj/uppmax2024-2-11/nobackup/$USER/nextflow_lab1
```

Load Nexflow

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
module load bioinfo-tools
module load Nextflow/22.10.1
```

If because of any reason, you cannot use UPPMAX, Nextflow can be installed on any POSIX compatible system for example Linux, OS X, Windows Subsystem for Linux. Head to [Nextflow website](https://www.nextflow.io/docs/latest/getstarted.html#installation) and follow the installation procedure.

# Basic concepts

Nextflow is a reactive workflow framework and a programming DSL that eases the writing of data-intensive computational pipelines.

It is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.

Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model.

## Nextflow scripting 

The Nextflow scripting language is an extension of the Groovy programming language. Groovy is a powerful programming language for the Java virtual machine. The Nextflow syntax has been specialized to ease the writing of computational pipelines in a declarative manner.

Nextflow can execute any piece of Groovy code or use any library for the JVM platform.

For example,



```{r attr.source='.numberLines',eval=FALSE, class.source="nohide"}
println "Hello, World!" // #1
 
x = 1 // #2
println x
 
x = new java.util.Date() // #2
println x
 
x = -3.1499392 // #2
println x
 
x = false // #2
println x
 
x = "Hi" // #2
println x

myList = [1776, -1, 33, 99, 0, 928734928763] // #3
println myList

square = { it * it } // #4
println square(9)
 
printMapClosure = { key, value ->
   println "$key = $value"
} // #4

map_example=[ "Yue" : "Wu", "Mark" : "Williams", "Sudha" : "Kumari" ] // #5


[ "Yue" : "Wu", "Mark" : "Williams", "Sudha" : "Kumari" ].each(printMapClosure) // #6


```

1. To print something is as easy as using one of the print or println methods
2. To define a variable, simply assign a value to it
3. A List object can be defined by placing the list items in square brackets
4. A closure is a block of code that can be passed as an argument to a function. Thus, you can define a chunk of code and then pass it around as if it were a string or an integer
5. Maps are used to store associative arrays or dictionaries. They are unordered collections of heterogeneous, named data
6. the method Map.each() can take a closure with two arguments, to which it binds the key and the associated value for each key-value pair in the Map

To test this, Create a file called `main_1.nf` using your favorite editor (i use nano)

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_1.nf
```

Copy and paste the script above. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Run the following command:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_1.nf
```

There are many other things that can be done with Groovy scripts. Please have a look at [Nextflow scripting
](https://www.nextflow.io/docs/latest/script.html#)

## Processes and channels

In practice a Nextflow pipeline script is made by joining together different `processes`. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).

Processes are executed independently and are isolated from each other, i.e. they do not share a common (writable) state. The only way they can communicate is via asynchronous FIFO queues, called `channels` in Nextflow.

Any process can define one or more channels as input and output. The interaction between these processes, and ultimately the pipeline execution flow itself, is defined by workflow declaration.


# Channels

We start with `channels` as they are more resemble the variable in a typical programming language. Nextflow is based on the Dataflow programming model in which processes communicate through channels. Using these channels we can connect different processes together. 

There are two different types of channel in Nextflow:

1. A queue channel is a non-blocking unidirectional FIFO queue which connects two processes or operators. *The same queue channel cannot be used more than one time as*
2. A value channel a.k.a. singleton channel by definition is bound to a single value and it can be read unlimited times without consuming its content.

We are going to focus on queue channels here. A queue channel is usually created using a factory method such as a from, fromPath, etc.

## of

The `of` method allows you to create a channel emitting any sequence of values that are specified as the method argument

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
println "Channel.of( 1, 3, 5, 7 ):"
ch = Channel.of( 1, 3, 5, 7 ) // #1 
ch.view()

println "Channel.of( [1, 3, 5, 7, 9]):"
ch = Channel.of( [1, 3, 5, 7, 9]) // #2
ch.view()

```

1. Creates a channel from a sequence of values and set the name to ch
2. Creates a channel from a list of values

Create a file called `main_2.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_2.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_2.nf
```


## fromPath

You can create a channel emitting one or more file paths by using the fromPath method and specifying a path string as an argument.


```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
// single file
myFileChannel = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/Blank10.mzML' ) // #1
myFileChannel.view()
// multiple files
myFileChannel = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.*' ) // #2
myFileChannel.view()
// recursive multiple files
myFileChannel = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/**.*' ) // #3
myFileChannel.view()

```

1. Creates a channel and binds to it a Path item referring the specified file.
2. Whenever the `fromPath` argument contains a `*` or `?` wildcard character it is interpreted as a `glob` path matcher.
3. Two asterisks, i.e. `**`, works like `*` but crosses directory boundaries.

Create a file called `main_3.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_3.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_3.nf
```


There are many parameters as well as channel types that can be used for different purposes. Please check the (documentation)[https://www.nextflow.io/docs/latest/channel.html].


# Processes

In Nextflow a process is the basic processing primitive to execute a user script. As said before, this processes can pretty much run anything from R scripts to complex bash commands. Anything that is executable on the linux system can be run!

The process definition starts with keyword the process, followed by process name and finally the process body delimited by curly brackets. The process body must contain a string which represents the command or, more generally, a script that is executed by it.

The overall structure of a process in Nextflow looks like this:

```{r attr.source='.numberLines',eval=FALSE, class.source="nohide"}
process < name > {

   [ directives ] // #1

   input:
    < process inputs > // #2

   output:
    < process outputs > // #3

   when:
    < condition > // #4

   [script|shell|exec]:
   < user script to be executed > // #5

}
```

1. Using the directive declarations block you can provide optional settings that will affect the execution of the current process
2. The input block defines from which channels the process expects to receive data.
3. The output declaration block allows you send out the results produced to the channels. 
4. The `when` declaration allows you to define a condition that must be verified in order to execute the process. 
5. The script block is a string statement that defines the command that is executed by the process to carry out its task.

## script|shell|exec

We start with the `script` block. A process contains one and only one script block, and it must be the last statement when the process contains input and output declarations. The entered string is executed as a Bash script in the host system. It can be any command, script or combination of them, that you would normally use in terminal shell or in a common Bash script. The script block can be a simple string or multi-line string. The latter simplifies the writing of non trivial scripts composed by multiple commands spanning over multiple lines. If you would like to run shell instead of bash you can use single quotation (`'''`) instead of double. 

For example,

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
db='database' // #1

process justEchoBash {
debug true // #2
 script:
 """
 echo $db // #3
 """

}

process justEchoShell {
debug true
   shell:
   '''
   echo !{db} // #4
   '''
}

workflow {
    justEchoBash()
    justEchoShell()
}

```

1. Creates a variable and assign it to `database`. 
2. This is a directive that is setting the process to print the standard output. More on this later!
3. Runs the bash command `echo` which prints the content of variable `db`. Note that the variables that you define outside of the process script can be accessed using `$` sign
4. This is identical to bash but using shell. To access the variable `db` we need to wrap in `!{}`

Please note that unless really needed try to use `bash`. 

Create a file called `main_4.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_4.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_4.nf
```


## input

The input block defines from which channels the process expects to receive data. You can only define one input block at a time and it must contain one or more input declarations.

The input block follows the syntax shown below:


```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
input:
  <input qualifier> <input name> 
```

An input definition starts with an input qualifier and then the input name.

The input qualifier declares the type of data to be received. 

The qualifiers available are the ones listed in the following table:

| Qualifier | Semantic                                                                                          |
|-----------|---------------------------------------------------------------------------------------------------|
| val       | Lets you access the received input value by its name in the process script.                       |
| env       | Lets you use the received value to set an environment variable named as the specified input name. |
| file      | Lets you handle the received value as a file, staging it properly in the execution context.       |
| path      | Lets you handle the received value as a path, staging the file properly in the execution context. |
| stdin     | Lets you forward the received value to the process stdin special file.                            |
| tuple     | Lets you handle a group of input values having one of the above qualifiers.                       |
| each      | Lets you execute the process for each entry in the input collection.                              |

For example, here we have a channel and a process that gets an input from the channel and prints the values.

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
num = Channel.from( 1, 2, 3 ) // #1

process basicExample {
debug true
 input:
 val x // #2
 "echo process job $x" // #3
}

workflow {           // #4
    basicExample(num)
}

```

Another way of writing the workflow when there is only one input. Here `num` is the only input channel.

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}

workflow {           // #5
    num | basicExample
}

```

1. Creates a channel of 1,2,3
2. Set the input of the process from the channel `num`. The type of the channel is `val`.
3. Runs a simple echo command that writes the value to std out! Remember that to access the input we need to use `$x`.
4. Executes the workflow containing the process basicExample with `num` as the input channel
5. When the process declares exactly one input, the pipe | operator can be used to provide inputs to the process, instead of passing it as a parameter. (similar to what you learned in Bash piping)

In the above example the process is executed three times, each time a value is received from the channel num and used to process the script. 

Create a file called `main_5.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_5.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_5.nf
```

Can you create a workflow that reads all the files with `*.mzML` extension in a file channel and print their name in a process? Remember, this is a file channel!

```{bash, attr.source='.numberLines',eval=FALSE}
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' )

process featureFinder {
debug true
 input:
 file mzML

 """
echo i’m processing $mzML file!
"""
}

workflow {
    featureFinder(mzMLFiles)
}

```

Create a file called `main_6.nf` and put write the code.

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_6.nf
```

Save the file (Ctrl+o enter) and exit (Ctrl+x). Now run 
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_6.nf
```

### input each

The each qualifier allows you to repeat the execution of a process for each item in a collection, every time a new data is received. 


```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' ) // #1
num = Channel.from( 1, 2, 3 ) // #2
process featureFinder {
  debug true
input:
each x  // #3
file y  // #4

"echo value $x file $y" // #5

}

workflow {
    featureFinder(num,mzMLFiles) // #6
}

```

1. Creates a file channel from all `mzML` files in `/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML'`.
2. Create a channel containing numbers
3. Define input repeater for the values in the channel
4. Define another input channel from the file channel 
5. Script that is executed
6. Workflow with the process featureFinder receving channels `num` and `mzMLFiles`. The order of the input has to be maintained to the order in which the `input` is written in the process. Here `x` receives value from `num` and `y` receives value from `mzMLFiles`.

In the above example every time a file (`y`) of `mzML` is received as input by the process, it executes three tasks running a `echo` with a different value for the x parameter. This is useful when you need to repeat the same task for a given set of parameters.

Create a file called `main_7.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_7.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_7.nf
```

## Output

The output declaration block allows you to define the channels used by the process to send out the results produced. You can only define one output block at a time and it must contain one or more output declarations.

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
output:
<output qualifier> <output name> [, <option>: <option value>]

```

The qualifiers that can be used in the output declaration block are the ones listed in the following table:

| Qualifier | Semantic                                                                                               |
|-----------|--------------------------------------------------------------------------------------------------------|
| val       | Sends variables with the name specified over the output channel.                                       |
| file      | Sends a file produced by the process with the name specified over the output channel.                  |
| path      | Sends a file produced by the process with the name specified over the output channel (replaces file).  |
| env       | Sends the variable defined in the process environment with the name specified over the output channel. |
| stdout    | Sends the executed process stdout over the output channel.                                             |
| tuple     | Sends multiple values over the same output channel.                                                    |


For example,

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' ) // #1
process featureFinder {
input:
file x  // #2
output:
file "output/${x.baseName}.featureXML" // #3

""" 
mkdir output 
cp -in $x output/${x.baseName}.featureXML 
"""  // #4
}

workflow {
    output_channel = featureFinder(mzMLFiles) // #5
    output_channel.view()
}

```

In the above example the process, when executed, it will create a file channel from `/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML`. The process will then get this channel as an input and creates an output channel `outputChannel` (named in the workflow) where each file extension has been changed to `featureXML`.

1. Creates a channel emitting files.
2. Receive value from the input channel.
3. Send data to output channel. The output file is located under `output` directory. `${x.baseName}` gives the name of the file without extension.
4. In the bash script, we first create an output folder We then copy the input to the output folder but change its extension. This is obviously a pretty useless command! But you can change this to a more meaningful one!
5. Sends the input channel to the process and receives the output from the process to a channel.

Create a file called `main_8.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_8.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_8.nf
```

Let's clean up the nextflow work directory now to free up some space

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow clean -f
```

## Directives

Using the directive declarations block you can provide optional settings that will affect the execution of the current process.

They must be entered at the top of the process body, before any other declaration blocks (i.e. input, output, etc) and have the following syntax:

You can see the complete list of directives [here](https://www.nextflow.io/docs/latest/process.html#directives)


### publishDir

The `publishDir` directive allows you to publish the process output files to a specified folder. 


```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
process foo {

    publishDir 'data/chunks' // #1

    output:
    file 'chunk_*'

    '''
    printf 'Hola' | split -b 1 - chunk_
    '''
}

workflow {
    foo()
}
```

The above example splits the string Hola into file chunks of a single byte. 

1. When complete the chunk_* output files are published into the /data/chunks folder.

*Can you create a workflow having a single process that just creates a text file (with whatever content) as an output and also publish its output to a directory?*

```{bash, attr.source='.numberLines',eval=FALSE}
process simpleOutput {
publishDir 'testOutput'
output:
file "test.txt"
"echo test >> test.txt"

}

workflow {
    simpleOutput()
}
```

Create a file called `main_9.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_9.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_9.nf
```

### tag

The `tag` directive allows you to associate each process execution with a custom label, so that it will be easier to identify them in the log file or in the trace execution report.


```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' )
num = Channel.from( 1, 2, 3 )
process featureFinder {
tag "$y" // #1
input:
each x
file y

"""
echo value $x file $y
"""

}

workflow {
    featureFinder(num,mzMLFiles)
}
```

In the above example, when a file is received by the process, it will show its name when running the process.

1. `$y` in the tag, indicates that name of file from `mzMLFiles` should be used as tag.

Create a file called `main_10.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_10.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_10.nf
```

What do you see? What is the difference to a process without a tag?

Now let's clean up the cache produced by Nextflow using the clean command 
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow clean -f
```
This removes all the cache files and saves space on the disk.

# Workflow
Workflow connects process and channels together. The dataflow logic is executed here. The workflow definition starts with the keyword `workflow`, followed by an optional name, and finally the workflow body delimited by curly braces. The logic is written inside the curly bracket. The order in which a task is executed is determined only by its dependencies, so a task will be executed as soon as all of its required inputs are available.  
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
workflow workflowname{
output_channel1 = example_workflow()
output_channel2 = example_workflow2(output_channel1)
output_channel2.view()
}
```

Channels can be passed and received in the workflow using
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
workflow {
output_channel = example_workflow(input_channel)
}
```

Operators can be applied to the input or output channel. For example
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
workflow {
output_channel = example_workflow(input_channel).collect() // #1
}
```
1. Operator `collect()` is applied to the data coming from the process `example_workflow`

Multiple channels can be provided as input and also received as
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
workflow {
(output_channel1,output_channel2) = example_workflow(input_channel1,input_channel2)
}
```


## Subworkflows
A named workflow is a “subworkflow” that can be invoked from other workflows. For example:
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
workflow my_pipeline { // #1
    foo()
    bar( foo.out.collect() )
}

workflow {
    my_pipeline()
}
```

1. Name of the subworkflow here is `my_pipeline`


# Operators

Nextflow operators are methods that allow you to connect channels to each other or to transform values emitted by a channel applying some user provided rules. There is a large number of operators that can be seen [here](https://www.nextflow.io/docs/latest/operator.html). We go through some of them!

## Collect

The `collect` operator collects all the items emitted by a channel to a List and return the resulting object as a *sole* emission.

For example,


```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' )
mzMLFiles.collect().view()

```

The above code, will get the files from the path and emit them all at once. You can try this and compare the results to when you don't use collect.


Create a file called `main_11.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_11.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_11.nf
```

This operator can also be used inside the process. For example,


```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' )
process featureFinder {
debug true

input:
file x

""" 
echo $x
"""  
}

workflow {
    featureFinder(mzMLFiles.collect()) // #1
}

```

1. By using `collect()`, the process will gather all the files in `mzMLFiles` channel and do the operation for this collection of files.

Run this example and compare the results to when you don't use collect. What is the difference? How many times the process will be run if you use and don't use collect?

## flatten

The `flatten` operator transforms a channel in such a way that every item of type Collection or `Array` is flattened so that each single entry is emitted separately by the resulting channel. 

for example,


```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
Channel
    .from( [1,[2,3]], 4, [5,[6]] )
    .flatten()
    .view()

```

Try to run this example and compare the results to collect!

# Configuration file


When a pipeline script is launched, Nextflow looks for configuration files in multiple locations. Since each configuration file can contain conflicting settings, the sources are ranked to decide which settings to are applied. All possible configuration sources are reported below, listed in order of priority:

1. Parameters specified on the command line (--something value)

2. Parameters provided using the -params-file option

3. Config file specified using the -c my_config option

4. The config file named nextflow.config in the current directory

5. The config file named nextflow.config in the workflow project directory

6. The config file $HOME/.nextflow/config

7. Values defined within the pipeline script itself (e.g. main.nf)

It is easy to write the config file. It is just a text file wherein you can assign variables. These variables are readily available in the pipeline to use. The comments should be written using `//`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
propertyOne = 'world'
anotherProp = "Hello $propertyOne"
customPath = "$PATH:/my/app/folder"
// comment!

```

## Config scopes

Configuration settings can be organized in different scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation.

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
alpha.x  = 1
alpha.y  = 'string value..'

beta {
     p = 2
     q = 'another string ..'
}
```

There are many important default scopes in Nextflow. You can have a look [here](https://www.nextflow.io/docs/latest/config.html#config-scopes).

### Scope params

The params scope allows you to define parameters that will be accessible in the pipeline script. Simply prefix the parameter names with the params scope or surround them by curly brackets. 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
params {

   mzMLFilesInput = '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' // #1
  ppm_input=10 // #2

}

```

In the example above, we define two parameters:

1. `mzMLFilesInput` which points to the location our input files.
2. `ppm_input` which is used in a bash script

We can then use these two parameters anywhere inside our pipeline. For example:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath( params.mzMLFilesInput ) // #1
process featureFinder {
debug true
input:
file x

"echo processing $x with $params.ppm_input ppm" // #2

}

workflow {
    featureFinder(mzMLFiles)
}
```

In this example:
1. We created a file channel using our parameter `params.mzMLFilesInput`
2. We accessed value of `ppm_input` using `$param` before the name of the parameter. 

Try the above code! Create two files, `main_12.nf`  and `nextflow_12.config`. and run the above example!

Pipeline script can use an arbitrary number of parameters that can be *overridden* either using the command line or the Nextflow configuration file. Any script parameter can be specified on the command line prefixing the parameter name with double dash characters e.g.:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow run <my script> --foo Hello
```

Then, the parameter can be accessed in the pipeline script using the params.foo identifier.

How would you run the above script so that `ppm_input` is send to the pipeline the command line? 

```{bash, attr.source='.numberLines',eval=FALSE}
nextflow run main_12.nf -c  nextflow_12.config --ppm_input 20
```

What happend with the default value of `ppm_input`?

### Scope process

The process configuration scope allows you to provide the default configuration for the processes in your pipeline.

You can specify here any property described in the [process directive](https://www.nextflow.io/docs/latest/process.html#process-directives) and the executor sections. For example,

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
process {
executor = 'slurm' // #1
  clusterOptions = { "-A $params.project ${params.clusterOptions ?: ''}" } // #2
}
```

In the example above:
1. We set the `executor` to `slurm` so that all the jobs will be send to `slurm`
2. We set some addition parameters for example `-A` for setting the project ID and `clusterOptions` for additional arguments


### Scope executor

The executor configuration scope allows you to set the optional executor settings.
The executor settings can be defined as shown below:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
executor {
    name = 'slurm'
    queueSize = 200
    clusterOptions = { "-A $params.project ${params.clusterOptions ?: ''}" }
}
```

When using two (or more) different executors in your pipeline, you can specify their settings separately by prefixing the executor name with the symbol `$` and using it as special scope identifier. For example:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
executor {
  $slurm {
    queueSize = 200
    clusterOptions = { "-A $params.project ${params.clusterOptions ?: ''}" }
  }

  $local {
      cpus = 8
      memory = '32 GB'
  }
}
```

### Configuration on UPPMAX

UPPMAX uses slurm job scheduler. We can use the process scope to instruct Nextflow to use slurm

Create a file called `nextflow_13.config` with the following content:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
process.executor = 'slurm'
process.clusterOptions = { "-A $params.project ${params.clusterOptions ?: ''}"}
```

Create a file called `main_13.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
// This is main.nf
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' )
process featureFinder {
debug true
input:
file x

"echo processing $x"

}

workflow {
    featureFinder(mzMLFiles)
}
```

You can now run the pipeline using

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_13.nf -c nextflow_13.config --project "uppmax2024-2-11" --clusterOptions "-M snowy"
```

The process is now running on Uppmax.
OK! This is probably going to take time. Use `Ctrl+c` to kill Nextflow! Then cancel all of your jobs!

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
scancel -u $USER -M snowy
```


### Containers

The docker configuration scope controls how Docker containers are executed by Nextflow

For example 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
process.container = 'nextflow/examples' // #1

docker {
    enabled = true // #2
}
```

In the example above:

1. We first set `process.container` to the location of the Docker image for example in Dockerhub.
2. We enable Docker. 

All the processes will then use Docker container.

Similarly we can use singularity. 

Please be aware that if you Nextflow to use a Docker image and convert to singularity image. This process will take time. In order to save you time we have already done the conversion. To use what we have built, first run the following command:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
export NXF_SINGULARITY_CACHEDIR=/crex/proj/uppmax2024-2-11/metabolomics/singularity
```

The above command will set the location of the image for Nextflow. 

We can later create a file named `nextflow_14.config` with the following content

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
singularity.enabled = true // #1
process.container="bigdatacourse.sif" // #2
```

1. Instruct Nextflow to use Singularity
2. Sets the name of the image

We can now create another file ` main_14.nf` with the following content:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' )
process featureFinder {
debug true
input:
file x

"echo processing $x"

}

workflow {
    featureFinder(mzMLFiles)
}
```

Now we can run it using?

```{bash, attr.source='.numberLines',eval=FALSE}
nextflow main_14.nf -c nextflow_14.config 
```

#### Configuration singularity on Uppmax

Can you combine the Singularity and Slurm together to run containers using slurm? What config file would you create?! How would you run the Nextflow pipeline? Remember to kill Nextflow! 'Then cancel all of your jobs after you are done!'

```{bash, attr.source='.numberLines',eval=FALSE}
singularity.enabled = true
process.container="bigdatacourse.sif"
process.executor = 'slurm'
process.clusterOptions = { "-A $params.project ${params.clusterOptions ?: ''}"}
```


```{bash, attr.source='.numberLines',eval=FALSE}
mzMLFiles = Channel.fromPath('/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' )
process featureFinder {
debug true
input:
file x

"echo processing $x"

}

workflow {
    featureFinder(mzMLFiles)
}
```



```{bash, attr.source='.numberLines',eval=FALSE}
nextflow main_15.nf -c nextflow_15.config --project "uppmax2024-2-11" --clusterOptions "-M snowy"
```

Now let us cancel all the jobs

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
scancel -u $USER --state=pending
scancel -u $USER -t running
```

### Config profiles

Configuration files can contain the definition of one or more `profiles`. A profile is a set of configuration attributes that can be activated/chosen when launching a pipeline execution by using the `-profile` command line option.

Configuration profiles are defined by using the special scope profiles which group the attributes that belong to the same profile using a common prefix.

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
profiles { 

    standard { // #1
        process.executor = 'local' // #2
    }

    uppmax_singularity { // #3
process.executor = 'slurm' // #4
  	process.clusterOptions = { "-A $params.project ${params.clusterOptions ?: ''}"}
singularity.enabled = true // #5
 	}
}

```

Save the above in a file as `nextflow.config` in the directory where the `main.nf` is kept.

In this example, we have created two profiles.

1. `standard` uses `local` (#2) executor which means that the jobs will be running on the local environment if this profile is activated
3. `uppmax_singularity` uses Slurm and Singularity for running jobs on UPPMAX. 


Now if we want to run the pipeline on the local environment we use:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main.nf -profile standard
```

How would you run it if you wanted to run on UPPMAX?

```{bash, attr.source='.numberLines',eval=FALSE}
nextflow main.nf -profile uppmax_singularity --project "uppmax2024-2-11" --clusterOptions "-M snowy"
```

# More examples:

Here they are more examples of a bit more complex pipelines

## Two processes

The pipelines are connected by channels and the order is defined in the workflow


```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' )
process featureFinder {
input:
file x
output:
file "${x.baseName}.featureXML" // #1

"cp $x ${x.baseName}.featureXML"

}

// This process gets input from featureFinder
process someotherprocess {
debug true
input:
file x // #2

"echo I got $x from featureFinder"

}

workflow {
    featureFinder_out = featureFinder(mzMLFiles) // #1
    someotherprocess(featureFinder_out) // #2
}

```

In this example, the process `featureFinder` sends the output and received by an output channel (#1) which is then sent to `someotherprocess` to do further processing.


Create a file called `main_16.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_16.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_16.nf
```

## Two processes including collect

As mentioned above we can instruct Nextflow to emit all the content of a channel in a single emittion so that the downstream process will receive all the content at once using the `collect` operator. For example

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath( '/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML' )
process featureFinder {
input:
file x 
output:
file "${x.baseName}.featureXML" // #1

"cp $x ${x.baseName}.featureXML"
}

// This process gets input from featureFinder
process someotherprocess {
debug true
input:
file x

"echo I got $x from featureFinder"

}

workflow {
    featureFinder_out = featureFinder(mzMLFiles).collect() // #1
    someotherprocess(featureFinder_out) // #2
}
```

In this example, the process `featureFinder` sends output to an output channel (featureFinder_out) (#1) which is then sent to `someotherprocess` (#2) to do further processing. However, `someotherprocess` now has all the data at once because.


Create a file called `main_17.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_17.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_17.nf
```

What is the difference between this and the previous example?

## Two processes, collect, accept parameters from the user

We can now try to even accept some parameters from the user.

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath(params.inputmzml) // #1
process featureFinder {
input:
file x
output:
file "${x.baseName}.featureXML"
"cp $x ${x.baseName}.featureXML"

}

// This process gets input from featureFinder
process someotherprocess {
debug true
input:
file x

"echo I got $x from featureFinder"

}

workflow {
    featureFinder_out = featureFinder(mzMLFiles).collect()
    someotherprocess(featureFinder_out)
}

```

In this example, we create the `mzMLFiles` channel using the input we are getting from the user (#1).

Create a file called `main_18.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_18.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_18.nf --inputmzml "/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML"
```

Notice how we are sending parameters to the workflow!

## Three processes

Similarly we can create as many as processes we would like to. For example three:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mzMLFiles = Channel.fromPath(params.inputmzml) // #1
process justcopy {
input:
file x
output:
file "${x.baseName}.featureXML"
"cp $x ${x.baseName}.featureXML"
}
// This process gets input from justcopy
process someotherprocess {
debug true
input:
file x
output:
file "QC.txt"
"""
echo "${params.qc} percent!">> QC.txt 
"""
}
// This process gets input from someotherprocess
process lastone {
debug true
input:
file x
"""
(echo "Quality was:" ;cat $x)
"""
}

workflow {
    justcopy_out = justcopy(mzMLFiles)
    someotherprocess_out = someotherprocess(justcopy_out.collect())
    lastone(someotherprocess_out)
}

```

In this example, we create the `mzMLFiles` channel using the input we are getting from the user (#1). We are also getting another parameter from the user. Can you find it?


Create a file called `main_19.nf`

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nano main_19.nf
```

Copy the above code to it. Save the file `(Ctrl+o enter)` and exit `(Ctrl+x)`
Now run:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_19.nf --inputmzml "/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML" --qc 100
```

Now let's clean up the cache produced by Nextflow using the clean command 
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow clean -f
```

# Resuming the pipeline (Command line interface)

Nextflow provides a powerful command line interface for the execution pipelines. 

As we have seen before a general way of running Nextflow looks like:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow [options] COMMAND [arg...]
```

We often run the workflow using 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main_19.nf --inputmzml "/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML" --qc 100
```

However, when Nextflow is run it will automatically use its powerful `run` command. So the above code can actually be run using:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow run main_19.nf --inputmzml "/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML" --qc 100
```

The `run` command has several options that can be seen [here](https://www.nextflow.io/docs/latest/cli.html#run)

One the most important option is `resume`. This option executes the script using the cached results, useful to continue executions that was stopped by an error.

So if we imagine the above pipeline fails for example because of an error in one of the processes, after resolving the error, we can run the workflow using:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow run main_19.nf -resume --inputmzml "/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/*.mzML" --qc 100
```

Using this command, the already successfully executed processes will not be rerun. So the pipeline will continue from where it failed last time.

You can see the full description of Nextflow CLI [here](https://www.nextflow.io/docs/latest/cli.html)

# Assignment 2: Nextflow

Here we have two parts for the assignment to complete the Nextflow section of the course.

## Set up the environment

You need to do this every time you start the lab.

First ssh to Uppmax

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
ssh -AX youusername@rackham.uppmax.uu.se
```

Make a directory in your userspace

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mkdir -p /crex/proj/uppmax2024-2-11/nobackup/$USER/nextflow_lab
```

Navigate to the folder you have created

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
cd /crex/proj/uppmax2024-2-11/nobackup/$USER/nextflow_lab
```

Load Nexflow

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
module load bioinfo-tools
```

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
module load Nextflow/22.10.1
```

Fix some environmental variables:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
export NXF_SINGULARITY_CACHEDIR=/crex/proj/uppmax2024-2-11/metabolomics/singularity
```

You are now ready to start!

**The container for both of the following parts is “metaboigniter/course_docker:v5” and we want to use SLURM to run the jobs. **

**IMPORTANT: DO NOT remove, change or add anything here and in its subfolders: ** /crex/proj/uppmax2024-2-11/metabolomics  


*For part 1, please send us the modified main.nf, nextflow.config, and the modified command used to run (included in a file named command.txt).*
*For part 2, you will need to send us the main.nf, nextflow.config, and the PCA plot (along with any additional scripts). Submission of a PCA plot contributes to earning higher grades.* 

**IMPORTANT: Some time Uppmax file system might cause some of the processes to fail, if this happens, rerun the workflow with -resume option. This might actually pass the step that was previously shown as failed!**

**Use -resume whenever you run the workflow to skip the steps and (save both computational and your time ) which were completed before**  
For example
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow yourpipeline.nf -profile uppmax --project "uppmax2024-2-11" --clusterOptions "-M snowy" -resume
```


## Part 1

**For part 1, please send us the modified main.nf, nextflow.config, and the modified command used to run (included in a file named command.txt).**

In this part, we will run a small pipeline together. The pipeline is already in a folder that you need to copy to your personal space. We assume that you are now in `nextflow_lab`

Copy the pipeline and the configuration file

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
cp -r /crex/proj/uppmax2024-2-11/metabolomics/xcms_pipeline .
```

Now you have all the required files in your folder. Try to run the pipeline

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
cd xcms_pipeline 
```

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow main.nf -profile uppmax --project "uppmax2024-2-11" --clusterOptions "-M snowy"
```

You can open another terminal window and ssh to Uppmax and use jobinfo to see whether your jobs are running or not! If you realized that won’t run for some steps, it might be because of RAM, CPU, or running time. In the nextflow.config, you can change these parameters!

You can the jobinfo by logging in to uppmax via ssh using another termial.
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
jobinfo -u $USER -M snowy
```

Please remember that there is a high chance that Uppmax becomes very slow because the jobs are heavy. So please consider canceling your jobs if you already know and have a feeling about how Uppmax runs your jobs

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
scancel -u $USER -M snowy
```


*   Exercise one: There are some hardcoded parameters part of the pipeline:
    *   If process process_masstrace_detection_pos_xcms we have the following parameters:

        `peakwidthLow=5`


        `peakwidthHigh=10`


        `noise=1000`


        `polarity=positive`


**Can you change the config file so the value of these parameters is set there and fetched here?**
**Show us how the user can pass these parameters while running nextflow command? Show the command**

Tip: Your task is to modify the config file (nextflow.config) so that for example **peakwidthLow=5** can be replaced by **peakwidthLow=$params.peakwidthLow **and the value of **peakwidthLow** is set in the config file!

Help me: [params scope](https://www.nextflow.io/docs/latest/config.html#scope-params)


## Part 2
*For part 2, you will need to send us the main.nf, nextflow.config, and the PCA plot (along with any additional scripts). Submission of a PCA plot contributes to earning higher grades.*  

As talked before the pipeline in the part 1 does mass spectrometry data pre-processing using XCMS software suite. In this part of the exercise, you will need to build a pipeline from scratch that does the same thing using OpenMS. You can of course use the pipeline in part 1 as a template and try to modify it to do that. 


**Tip: Use echo to print the shell command executed by the nextflow, to verify the input and output are of desired format. You can pass "-debug true" to the nextflow command or have a directive "debug true" under a process to print for the respective process.**

When running your pipeline, remember to use these flags and the config file from the part1 `-profile uppmax --project "uppmax2024-2-11" --clusterOptions "-M snowy"`.
For example
```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
nextflow yourpipeline.nf -profile uppmax --project "uppmax2024-2-11" --clusterOptions "-M snowy"
```


### Part 2.1 (Build an OpenMS pipeline)

This is what you need to do



* Since the process in this specific pipeline need a separate parameter file you will need three additional **file** channels
    1. The first one is used in the FeatureFinder process and is located here: `/crex/proj/uppmax2024-2-11/metabolomics/openms_params/FeatureFinder.ini`
    2. The second one is for the alignment process and is located here: `/crex/proj/uppmax2024-2-11/metabolomics/openms_params/featureAlignment.ini`
    3. The third one is for the linker process and is located here: `/crex/proj/uppmax2024-2-11/metabolomics/openms_params/featureLinker.ini`
    
* You would also needall the  mzML files from this folder "/crex/proj/uppmax2024-2-11/metabolomics/mzMLData/" to a channel.

_Help me: [File channels](https://www.nextflow.io/docs/latest/process.html#input-of-files), [How to create file channels](https://www.nextflow.io/docs/latest/channel.html#frompath)_

Now that we have our channels up and running, time to create four processes:

1. Let’s name the first process `process_masstrace_detection_pos_OpenMS`
    1. This process has two inputs:
        1. The first input is from the mzML files
        2. The second one is from the **feature finder** parameter file. Please remember that The second input should be a type of input that repeats for **each** of the mzML file emitted by the first channel
    2. The output for this process has the same **baseName **as input but it has “.featureXML” extension. The name of the output channel must be alignmentProcess
    3. And finally, the command needed to run is 

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
FeatureFinderMetabo -in inputVariable -out inputBaseName.featureXML -ini settingFIleVariable
```


Please note that you need to change `inputVariable`, `inputBaseName` and `settingFIleVariable`! Please also note that the inputs to this tool are given by `-in`, the outputs by -out and settings by `-ini`.

Help me: [processes](https://www.nextflow.io/docs/latest/process.html), [input each (file)](https://www.nextflow.io/docs/latest/process.html#input-repeaters), [bash command](https://www.nextflow.io/docs/latest/process.html#script)



2. The second process is called `process_masstrace_alignment_pos_OpenMS`
    4. Similar to the previous process it will get two inputs:
        3. The first one is the output of `process_masstrace_detection_pos_OpenMS`
        4. The second one is from the alignment parameter file. Similar to the previous one this input is also repeating!
    5. The output for this process has the same `baseName` as input but it has `.featureXML` extension. However, these files are in a different folder (let’s call it `out`). The output channel must be named `LinkerProcess`
    6. The command needed is a bit different from the previous one:

This command accepts ALL the featureXML files at **the same time** (look at collect!) and outputs the same number of files. The inputs must be separated by space (remember the join operation in the XCMS pipeline?). The output is the same as input however you need to put the output in a different folder within the process! Think about a simple bash script. You create a folder and join the files in a way that they are written in a separate folder!  

Given three samples, x.featureXML, y.featureXML and z.featureXML, an example of the command is like

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
mkdir out
MapAlignerPoseClustering -in x.featureXML y.featureXML z.featureXMLL -out out/x.featureXMLL out/y.featureXML out/z.featureXML -ini setting.ini
```


**Please remember the code above is just an example of a command you run in bash You will have to implement this using Nextflow!**

Please also note that the inputs to this tool are given by `-in`, the outputs by `-out` and settings by `-ini`.

Finally, this process needs quite a bit of RAM to go forward. Set the memory for this to probably around 10 GB or even more!

Help me: [collect](https://www.nextflow.io/docs/latest/operator.html#collect), [joining in the process!](https://groups.google.com/g/nextflow/c/V7kKJzyZs98), [memory](https://www.nextflow.io/docs/latest/process.html#memory)


Need help!

Use the pseudo code given below for getting the desired input and output format.  
Use println function from groovy to check the format.

```{bash, attr.source='.numberLines',eval=FALSE, class.source="hide"}
process dummy_process{
input:
file filefromchannel

script:
def space_separated = filefromchannel.join(" ")
def string_separated = filefromchannel.join(" my_path/")

println (space_separated) 
println (string_separated)
}
```



3. The third process is called `process_masstrace_linker_pos_OpenMS` and is used to link (merge) multiple files into a single file
    7. It will get two inputs:
        5. The output from the alignment
        6. The parameter file
    8. The output of this process is a single file and must be named “Aggregated.consensusXML” it will be sent over to a channel called “textExport”
    9. And the command:

    This is similar to the previous step. You need to gather all the inputs separated by space (collect and join!). However, it will only output one file. Given the same three files above an example of the command would be:

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
FeatureLinkerUnlabeledQT -in x.featureXML y.featureXML z.featureXML -out Aggregated.consensusXML -ini setting.ini
```


Please also note that the inputs to this tool are given by `-in`, the outputs by -out and settings by `-ini`.

Finally, this process needs quite a bit of RAM to go forward. Set the memory for this to probably around `10 GB` or even more!



4. Finally the last step which takes the xml formatted output from the previous step and convert it into a csv file. Let’s call this process `process_masstrace_exporter_pos_OpenMS`
    10. The input to the process is the output from the previous step
    11. The output is a single file and must be called `Aggregated_clean.csv` and sent over to a channel called `out`
    12. To run the command: This process will run two commands the first one does the conversion and the second one the cleaning. We will only need the last output (`Aggregated_clean.csv`)

```{bash, attr.source='.numberLines',eval=FALSE, class.source="nohide"}
TextExporter -in input -out Aggregated.csv
/usr/bin/readOpenMS.r input=Aggregated.csv output=Aggregated_table.csv
```


Please also note that the inputs to this tool are given by `-in`, the outputs by `-out`.

Remember that this process should publish its result to a specific directory (your choice). It does that by symlink! Change the mode to copy if you want!

Help me: [publish the results](https://www.nextflow.io/docs/latest/process.html#publishdir)


### Part 2.2 PCA (optional, gives extra points)

Given the output from the last step of the pipeline, do a PCA on the data. The output of the last step is a csv file (comma separated) and the first column is the header. The missing values are designated by NA. 

Can you add your script (PCA) as an extra node part of the pipeline? You don’t have to use containers, you can use conda or whatever you like! There are R and Python already available in the container, you can also use [conda](https://www.nextflow.io/docs/latest/process.html#conda) or [modules](https://www.nextflow.io/docs/latest/process.html#module)

If you don’t know what PCA is or how to do it, please let us know!

**If you encounter issues with Uppmax being way too slow or does not do anything, please let up know!**
