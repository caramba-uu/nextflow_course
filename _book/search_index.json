[["index.html", "Introduction to Nextflow Chapter 1 Introduction 1.1 Setting up the environment", " Introduction to Nextflow 2023-04-14 Chapter 1 Introduction Welcome to this short tutorial on Nextflow. We are going to walk through some of the functionalities of this powerful workflow engine. More specifically we are going to do some hands-on work on processes, channels and operators. We are also going to look into how to configure Nextflow to run on different platforms. But before moving forward, we need to set up the environment for using Nextflow. 1.1 Setting up the environment We are going to use UPPMAX for doing most of the hands-on work. First ssh to UPPMAX ssh -AX youusername@rackham.uppmax.uu.se Make a directory in your user space (only if you have not done before) mkdir -p /crex/proj/uppmax2023-2-3/nobackup/$USER/nextflow_lab1 Navigate to the folder you have created cd /crex/proj/uppmax2023-2-3/nobackup/$USER/nextflow_lab1 Load Nexflow module load bioinfo-tools module load Nextflow/20.10.0 If because of any reason, you cannot use UPPMAX, Nextflow can be installed on any POSIX compatible system for example Linux, OS X, Windows Subsystem for Linux. Head to Nextflow website and follow the installation procedure. "],["basic-concepts.html", "Chapter 2 Basic concepts 2.1 Nextflow scripting 2.2 Processes and channels", " Chapter 2 Basic concepts Nextflow is a reactive workflow framework and a programming DSL that eases the writing of data-intensive computational pipelines. It is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations. Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model. 2.1 Nextflow scripting The Nextflow scripting language is an extension of the Groovy programming language. Groovy is a powerful programming language for the Java virtual machine. The Nextflow syntax has been specialized to ease the writing of computational pipelines in a declarative manner. Nextflow can execute any piece of Groovy code or use any library for the JVM platform. For example, println &quot;Hello, World!&quot; #1 x = 1 #2 println x x = new java.util.Date() #2 println x x = -3.1499392 #2 println x x = false #2 println x x = &quot;Hi&quot; #2 println x myList = [1776, -1, 33, 99, 0, 928734928763] #3 println myList square = { it * it } #4 println square(9) printMapClosure = { key, value -&gt; println &quot;$key = $value&quot; } #4 map_example=[ &quot;Yue&quot; : &quot;Wu&quot;, &quot;Mark&quot; : &quot;Williams&quot;, &quot;Sudha&quot; : &quot;Kumari&quot; ] #5 [ &quot;Yue&quot; : &quot;Wu&quot;, &quot;Mark&quot; : &quot;Williams&quot;, &quot;Sudha&quot; : &quot;Kumari&quot; ].each(printMapClosure) #6 To print something is as easy as using one of the print or println methods To define a variable, simply assign a value to it A List object can be defined by placing the list items in square brackets A closure is a block of code that can be passed as an argument to a function. Thus, you can define a chunk of code and then pass it around as if it were a string or an integer Maps are used to store associative arrays or dictionaries. They are unordered collections of heterogeneous, named data the method Map.each() can take a closure with two arguments, to which it binds the key and the associated value for each key-value pair in the Map To test this, Create a file called main_1.nf using your favorite editor (i use nano) nano main_1.nf Copy and paste the script above. Save the file (Ctrl+o enter) and exit (Ctrl+x) Run the following command: nextflow main_1.nf There are many other things that can be done with Groovy scripts. Please have a look at Nextflow scripting 2.2 Processes and channels In practice a Nextflow pipeline script is made by joining together different processes. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.). Processes are executed independently and are isolated from each other, i.e. they do not share a common (writable) state. The only way they can communicate is via asynchronous FIFO queues, called channels in Nextflow. Any process can define one or more channels as input and output. The interaction between these processes, and ultimately the pipeline execution flow itself, is implicitly defined by these input and output declarations. "],["channels.html", "Chapter 3 Channels 3.1 of 3.2 fromPath", " Chapter 3 Channels We start with channels as they are more resemble the variable in a typical programming language. Nextflow is based on the Dataflow programming model in which processes communicate through channels. Using these channels we can connect different processes together. There are two different types of channel in Nextflow: A queue channel is a non-blocking unidirectional FIFO queue which connects two processes or operators. The same queue channel cannot be used more than one time as A value channel a.k.a. singleton channel by definition is bound to a single value and it can be read unlimited times without consuming its content. We are going to focus on queue channels here. A queue channel is usually created using a factory method such as a from, fromPath, etc. 3.1 of The of method allows you to create a channel emitting any sequence of values that are specified as the method argument println &quot;Channel.of( 1, 3, 5, 7 ):&quot; ch = Channel.of( 1, 3, 5, 7 ) #1 ch.view() println &quot;Channel.of( [1, 3, 5, 7, 9]):&quot; ch = Channel.of( [1, 3, 5, 7, 9]) #2 ch.view() Creates a channel from a sequence of values and set the name to ch Creates a channel from a list of values Create a file called main_2.nf nano main_2.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_2.nf 3.2 fromPath You can create a channel emitting one or more file paths by using the fromPath method and specifying a path string as an argument. // single file myFileChannel = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/Blank10.mzML&#39; ) #1 myFileChannel.view() // multiple files myFileChannel = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.*&#39; ) #2 myFileChannel.view() // recursive multiple files myFileChannel = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/**.*&#39; ) #3 myFileChannel.view() Creates a channel and binds to it a Path item referring the specified file. Whenever the fromPath argument contains a * or ? wildcard character it is interpreted as a glob path matcher. Two asterisks, i.e. **, works like * but crosses directory boundaries. Create a file called main_3.nf nano main_3.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_3.nf There are many parameters as well as channel types that can be used for different purposes. Please check the (documentation)[https://www.nextflow.io/docs/latest/channel.html]. "],["processes.html", "Chapter 4 Processes 4.1 script|shell|exec 4.2 input 4.3 Output 4.4 Directives", " Chapter 4 Processes In Nextflow a process is the basic processing primitive to execute a user script. As said before, this processes can pretty much run anything from R scripts to complex bash commands. Anything that is executable on the linux system can be run! The process definition starts with keyword the process, followed by process name and finally the process body delimited by curly brackets. The process body must contain a string which represents the command or, more generally, a script that is executed by it. The overall structure of a process in Nextflow looks like this: process &lt; name &gt; { [ directives ] #1 input: &lt; process inputs &gt; #2 output: &lt; process outputs &gt; #3 when: &lt; condition &gt; #4 [script|shell|exec]: &lt; user script to be executed &gt; #5 } Using the directive declarations block you can provide optional settings that will affect the execution of the current process The input block defines from which channels the process expects to receive data. The output declaration block allows you to define the channels used by the process to send out the results produced. The when declaration allows you to define a condition that must be verified in order to execute the process. The script block is a string statement that defines the command that is executed by the process to carry out its task. 4.1 script|shell|exec We start with the script block. A process contains one and only one script block, and it must be the last statement when the process contains input and output declarations. The entered string is executed as a Bash script in the host system. It can be any command, script or combination of them, that you would normally use in terminal shell or in a common Bash script. The script block can be a simple string or multi-line string. The latter simplifies the writing of non trivial scripts composed by multiple commands spanning over multiple lines. If you would like to run shell instead of bash you can use single quotation (''') instead of double. For example, db=&#39;database&#39; #1 process justEchoBash { echo true #2 script: &quot;&quot;&quot; echo $db #3 &quot;&quot;&quot; } process justEchoShell { echo true shell: &#39;&#39;&#39; echo !{db} #4 &#39;&#39;&#39; } Creates a variable and assign it to database. This is a directive that is setting the process to print the standard output. More on this later! Runs the bash command echo which prints the content of variable db. Note that the variables that you define outside of the process script can be accessed using $ sign This is identical to bash but using shell. To access the variable db we need to wrap in !{} Please note that unless really needed try to use bash. Create a file called main_4.nf nano main_4.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_4.nf 4.2 input The input block defines from which channels the process expects to receive data. You can only define one input block at a time and it must contain one or more input declarations. The input block follows the syntax shown below: input: &lt;input qualifier&gt; &lt;input name&gt; [from &lt;source channel&gt;] [attributes] An input definition starts with an input qualifier and the input name, followed by the keyword from and the actual channel over which inputs are received. Finally some input optional attributes can be specified. The input qualifier declares the type of data to be received. The qualifiers available are the ones listed in the following table: Qualifier Semantic val Lets you access the received input value by its name in the process script. env Lets you use the received value to set an environment variable named as the specified input name. file Lets you handle the received value as a file, staging it properly in the execution context. path Lets you handle the received value as a path, staging the file properly in the execution context. stdin Lets you forward the received value to the process stdin special file. tuple Lets you handle a group of input values having one of the above qualifiers. each Lets you execute the process for each entry in the input collection. For example, here we are a channel and a process that gets an input the channel and prints the values. num = Channel.from( 1, 2, 3 ) #1 process basicExample { echo true input: val x from num #2 &quot;echo process job $x&quot; #3 } Creates a channel of 1,2,3 Set the input of the process to the channel num. The type of the channel is val. Runs a simple echo command that writes the value to std out! Remember that to access the input we need to use $x. In the above example the process is executed three times, each time a value is received from the channel num and used to process the script. Create a file called main_5.nf nano main_5.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_5.nf Can you create a workflow that reads all the files with *.mzML extension in a file channel and print their name in a process? Remember, this is a file channel! mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) process featureFinder { echo true input: file mzML from mzMLFiles &quot;&quot;&quot; echo i’m processing $mzML file! &quot;&quot;&quot; } Create a file called main_6.nf and put write the code. nano main_6.nf Save the file (Ctrl+o enter) and exit (Ctrl+x). Now run nextflow main_6.nf 4.2.1 input each The each qualifier allows you to repeat the execution of a process for each item in a collection, every time a new data is received. mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) #1 num = Channel.from( 1, 2, 3 ) #2 process featureFinder { echo true input: each x from num #3 file y from mzMLFiles #4 &quot;echo value $x file $y&quot; #5 } Creates a file channel from all mzML files in /crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML'. Create a value type channel Define input repeater for the value channel Define another input channel from the file channel In the above example every time a file (y) of mzML is received as input by the process, it executes three tasks running a echo with a different value for the x parameter. This is useful when you need to repeat the same task for a given set of parameters. Create a file called main_7.nf nano main_7.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_7.nf 4.3 Output The output declaration block allows you to define the channels used by the process to send out the results produced. You can only define one output block at a time and it must contain one or more output declarations. output: &lt;output qualifier&gt; &lt;output name&gt; [into &lt;target channel&gt;[,channel,..]] [attribute [,..]] The qualifiers that can be used in the output declaration block are the ones listed in the following table: Qualifier Semantic val Sends variables with the name specified over the output channel. file Sends a file produced by the process with the name specified over the output channel. path Sends a file produced by the process with the name specified over the output channel (replaces file). env Sends the variable defined in the process environment with the name specified over the output channel. stdout Sends the executed process stdout over the output channel. tuple Sends multiple values over the same output channel. For example, mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) #1 process featureFinder { input: file x from mzMLFiles #2 output: file &quot;output/${x.baseName}.featureXML&quot; into outputChannel #3 &quot;&quot;&quot; mkdir output cp -in $x output/${x.baseName}.featureXML &quot;&quot;&quot; #5 } outputChannel.view() In the above example the process, when executed, it will create a file channel from /crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML. The process will then get this channel as an input and creates an output channel outputChannel where each file extension has been changed to featureXML. Creates a channel emitting files. Use the created channel as an input to the process create output channel and put the output file in it. The output file is located under output directory. ${x.baseName} gives the name of the file without extension. In the bash script, we first create an output folder We then copy the input to the output folder but change its extension. This is obviously a pretty useless command! But you can change this to a more meaningful one! Create a file called main_8.nf nano main_8.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_8.nf 4.4 Directives Using the directive declarations block you can provide optional settings that will affect the execution of the current process. They must be entered at the top of the process body, before any other declaration blocks (i.e. input, output, etc) and have the following syntax: You can see the complete list of directives here 4.4.1 publishDir The publishDir directive allows you to publish the process output files to a specified folder. process foo { publishDir &#39;/data/chunks&#39; #1 output: file &#39;chunk_*&#39; into letters &#39;&#39;&#39; printf &#39;Hola&#39; | split -b 1 - chunk_ &#39;&#39;&#39; } The above example splits the string Hola into file chunks of a single byte. When complete the chunk_* output files are published into the /data/chunks folder. Can you create a workflow having a single process that just creates a text file (with whatever content) as an output and also publish its output to a directory? process simpleOutput { publishDir &#39;testOutput&#39; output: file &quot;test.txt&quot; into outputChannel &quot;echo test &gt;&gt; test.txt&quot; } 4.4.2 tag The tag directive allows you to associate each process execution with a custom label, so that it will be easier to identify them in the log file or in the trace execution report. mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) num = Channel.from( 1, 2, 3 ) process featureFinder { tag &quot;$y&quot; #1 input: each x from num file y from mzMLFiles &quot;&quot;&quot; echo value $x file $y &quot;&quot;&quot; } In the above example, when a file is received by the process, it will show its name when running the process. $y in the tag, indicates that name of file from mzMLFiles should be used as tag. Create a file called main_9.nf nano main_9.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_9.nf What do you see? What is the difference to a process without a tag? "],["operators.html", "Chapter 5 Operators 5.1 Collect 5.2 flatten", " Chapter 5 Operators Nextflow operators are methods that allow you to connect channels to each other or to transform values emitted by a channel applying some user provided rules. There is a large number of operators that can be seen here. We go through some of them! 5.1 Collect The collect operator collects all the items emitted by a channel to a List and return the resulting object as a sole emission. For example, mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) mzMLFiles.collect().view() The above code, will get the files from the path and emit them all at once. You can try this and compare the results to when you don’t use collect. Create a file called main_10.nf nano main_10.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_10.nf This operator can also be used inside the process. For example, mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) process featureFinder { input: file x from mzMLFiles.collect() #1 &quot;&quot;&quot; echo $x &quot;&quot;&quot; } By using collect(), the process will gather all the files in mzMLFiles channel and do the operation for this collection of files. Run this example and compare the results to when you don’t use collect. What is the difference? How many times the process will be run if you use and don’t use collect? 5.2 flatten The flatten operator transforms a channel in such a way that every item of type Collection or Array is flattened so that each single entry is emitted separately by the resulting channel. for example, Channel .from( [1,[2,3]], 4, [5,[6]] ) .flatten() .view() Try to run this example and compare the results to collect! "],["configuration-file.html", "Chapter 6 Configuration file 6.1 Config scopes", " Chapter 6 Configuration file When a pipeline script is launched, Nextflow looks for configuration files in multiple locations. Since each configuration file can contain conflicting settings, the sources are ranked to decide which settings to are applied. All possible configuration sources are reported below, listed in order of priority: Parameters specified on the command line (–something value) Parameters provided using the -params-file option Config file specified using the -c my_config option The config file named nextflow.config in the current directory The config file named nextflow.config in the workflow project directory The config file $HOME/.nextflow/config Values defined within the pipeline script itself (e.g. main.nf) It is easy to write the config file. It is just a text file wherein you can assign variables. These variables are readily available in the pipeline to use. The comments should be written using // propertyOne = &#39;world&#39; anotherProp = &quot;Hello $propertyOne&quot; customPath = &quot;$PATH:/my/app/folder&quot; // comment! 6.1 Config scopes Configuration settings can be organized in different scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation. alpha.x = 1 alpha.y = &#39;string value..&#39; beta { p = 2 q = &#39;another string ..&#39; } There are many important default scopes in Nextflow. You can have a look here. 6.1.1 Scope params The params scope allows you to define parameters that will be accessible in the pipeline script. Simply prefix the parameter names with the params scope or surround them by curly brackets. params { mzMLFilesInput = &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; #1 ppm_input=10 #2 } In the example above, we define two parameters: mzMLFilesInput which points to the location our input files. ppm_input which is used in a bash script We can then use these two parameters anywhere inside our pipeline. For example: mzMLFiles = Channel.fromPath( params.mzMLFilesInput ) #1 process featureFinder { echo true input: file x from mzMLFiles &quot;echo processing $x with $params.ppm_input ppm&quot; #2 } In this example: 1. We created a file channel using our parameter params.mzMLFilesInput 2. We accessed value of ppm_input using $param before the name of the parameter. Try the above code! Create two files, main_11.nf and nextflow_11.config. and run the above example! Pipeline script can use an arbitrary number of parameters that can be overridden either using the command line or the Nextflow configuration file. Any script parameter can be specified on the command line prefixing the parameter name with double dash characters e.g.: nextflow run &lt;my script&gt; --foo Hello Then, the parameter can be accessed in the pipeline script using the params.foo identifier. How would you run the above script so that ppm_input is send to the pipeline the command line? nextflow run main_11.nf -c nextflow_11.config --ppm_input 20 What happend with the default value of ppm_input? 6.1.2 Scope process The process configuration scope allows you to provide the default configuration for the processes in your pipeline. You can specify here any property described in the process directive and the executor sections. For example, process { executor = &#39;slurm&#39; #1 clusterOptions = { &quot;-A $params.project ${params.clusterOptions ?: &#39;&#39;}&quot; } #2 } In the example above: 1. We set the executor to slurm so that all the jobs will be send to slurm 2. We set some addition parameters for example -A for setting the project ID and clusterOptions for additional arguments 6.1.3 Scope executor The executor configuration scope allows you to set the optional executor settings. The executor settings can be defined as shown below: executor { name = &#39;slurm&#39; queueSize = 200 clusterOptions = { &quot;-A $params.project ${params.clusterOptions ?: &#39;&#39;}&quot; } } When using two (or more) different executors in your pipeline, you can specify their settings separately by prefixing the executor name with the symbol $ and using it as special scope identifier. For example: executor { $slurm { queueSize = 200 clusterOptions = { &quot;-A $params.project ${params.clusterOptions ?: &#39;&#39;}&quot; } } $local { cpus = 8 memory = &#39;32 GB&#39; } } 6.1.4 Configuration on UPPMAX UPPMAX uses slurm job scheduler. We can use the process scope to instruct Nextflow to use slurm Create a file called nextflow_12.config with the following content: process.executor = &#39;slurm&#39; process.clusterOptions = { &quot;-A $params.project ${params.clusterOptions ?: &#39;&#39;}&quot;} Create a file called main_12.nf // This is main.nf mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) process featureFinder { echo true input: file x from mzMLFiles &quot;echo processing $x&quot; } You can now run the pipeline using nextflow main_12.nf -c nextflow_12.config --project &quot;uppmax2023-2-3&quot; --clusterOptions &quot;-M snowy&quot; The process is now running on Uppmax. OK! This is probably going to take time. Use Ctrl+c to kill Nextflow! Then cancel all of your jobs! scancel -u $USER -M snowy 6.1.5 Containers The docker configuration scope controls how Docker containers are executed by Nextflow For example process.container = &#39;nextflow/examples&#39; #1 docker { enabled = true #2 } In the example above: We first set process.container to the location of the Docker image for example in Dockerhub. We enable Docker. All the processes will then use Docker container. Similarly we can use singularity. Please be aware that if you Nextflow to use a Docker image and convert to singularity image. This process will take time. In order to save you time we have already done the conversion. To use what we have built, first run the following command: export NXF_SINGULARITY_CACHEDIR=/crex/proj/uppmax2023-2-3/metabolomics/singularity The above command will set the location of the image for Nextflow. We can later create a file named nextflow_13.config with the following content singularity.enabled = true #1 process.container=&quot;metaboigniter/course_docker:v5&quot; #2 Instruct Nextflow to use Singularity Sets the name of the image We can now create another file main_13.nf with the following content: mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) process featureFinder { echo true input: file x from mzMLFiles &quot;echo processing $x&quot; } Now we can run it using? nextflow main_13.nf -c nextflow_13.config 6.1.5.1 configuration singularity on Uppmax Can you combine the Singularity and Slurm together to run containers using slurm? What config file would you create?! How would you run the Nextflow pipeline? Remember to kill Nextflow! Then cancel all of your jobs after you are done! singularity.enabled = true process.container=&quot;metaboigniter/course_docker:v5&quot; process.executor = &#39;slurm&#39; process.clusterOptions = { &quot;-A $params.project ${params.clusterOptions ?: &#39;&#39;}&quot;} mzMLFiles = Channel.fromPath(&#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) process featureFinder { echo true input: file x from mzMLFiles &quot;echo processing $x&quot; } nextflow main_14.nf -c nextflow_14.config --project &quot;uppmax2023-2-3&quot; --clusterOptions &quot;-M snowy&quot; 6.1.6 Config profiles Configuration files can contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated/chosen when launching a pipeline execution by using the -profile command line option. Configuration profiles are defined by using the special scope profiles which group the attributes that belong to the same profile using a common prefix. profiles { standard { #1 process.executor = &#39;local&#39; #2 } uppmax_singularity { #3 process.executor = &#39;slurm&#39; #4 process.clusterOptions = { &quot;-A $params.project ${params.clusterOptions ?: &#39;&#39;}&quot; singularity.enabled = true #5 } } In this example, we have created two profiles. standard uses local (#2) executor which means that the jobs will be running on the local environment if this profile is activated uppmax_singularity uses Slurm and Singularity for running jobs on UPPMAX. Now if we want to run the pipeline on the local environment we use: nextflow main.nf -profile standard How would you run it if you wanted to run on UPPMAX? nextflow main.nf -profile uppmax_singularity --project &quot;uppmax2023-2-3&quot; --clusterOptions &quot;-M snowy&quot; "],["more-examples.html", "Chapter 7 More examples: 7.1 Two processes 7.2 Two processes including collect 7.3 Two processes, collect, accept parameters from the user 7.4 Three processes", " Chapter 7 More examples: Here they are more examples of a bit more complex pipelines 7.1 Two processes As said before, you can connect two processes using channels. mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) process featureFinder { input: file x from mzMLFiles output: file &quot;${x.baseName}.featureXML&quot; into outputChannel #1 &quot;cp $x ${x.baseName}.featureXML&quot; } // This process gets input from featureFinder process someotherprocess { echo true input: file x from outputChannel #2 &quot;echo I got $x from featureFinder&quot; } In this example, the process featureFinder defines an output channel (#1) which is then read by someotherprocess to do further processing. Create a file called main_15.nf nano main_15.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_15.nf 7.2 Two processes including collect As mentioned above we can instruct Nextflow to emit all the content of a channel in a single emittion so that the downstream process will receive all the content at once using the collect operator. For example mzMLFiles = Channel.fromPath( &#39;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&#39; ) process featureFinder { input: file x from mzMLFiles output: file &quot;${x.baseName}.featureXML&quot; into outputChannel &quot;cp $x ${x.baseName}.featureXML&quot; } // This process gets input from featureFinder process someotherprocess { echo true input: file x from outputChannel.collect() #1 &quot;echo I got $x from featureFinder&quot; } In this example, the process featureFinder defines an output channel (#1) which is then read by someotherprocess to do further processing. However, someotherprocess now has all the data at once because. Create a file called main_16.nf nano main_16.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run nextflow main_16.nf What is the difference between this and the previous example? 7.3 Two processes, collect, accept parameters from the user We can now try to even accept some parameters from the user. mzMLFiles = Channel.fromPath(params.inputmzml) #1 process featureFinder { input: file x from mzMLFiles output: file &quot;${x.baseName}.featureXML&quot; into outputChannel &quot;cp $x ${x.baseName}.featureXML&quot; } // This process gets input from featureFinder process someotherprocess { echo true input: file x from outputChannel.collect() &quot;echo I got $x from featureFinder&quot; } In this example, we create the mzMLFiles channel using the input we are getting from the user (#1). Create a file called main_17.nf nano main_17.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run: nextflow main_17.nf --inputmzml &quot;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&quot; Notice how we are sending parameters to the workflow! 7.4 Three processes Similarly we can create as many as processes we would like to. For example three: mzMLFiles = Channel.fromPath(params.inputmzml) #1 process justcopy { input: file x from mzMLFiles output: file &quot;${x.baseName}.featureXML&quot; into outputChannel &quot;cp $x ${x.baseName}.featureXML&quot; } // This process gets input from justcopy process someotherprocess { echo true input: file x from outputChannel.collect() output: file &quot;QC.txt&quot; into finalchannel &quot;&quot;&quot; echo &quot;${params.qc} percent!&quot;&gt;&gt; QC.txt &quot;&quot;&quot; } // This process gets input from someotherprocess process lastone { echo true input: file x from finalchannel &quot;&quot;&quot; (echo &quot;Quality was:&quot; ;cat $x) &quot;&quot;&quot; } In this example, we create the mzMLFiles channel using the input we are getting from the user (#1). We are also getting another parameter from the user. Can you find it? Create a file called main_18.nf nano main_18.nf Copy the above code to it. Save the file (Ctrl+o enter) and exit (Ctrl+x) Now run: nextflow main_18.nf --inputmzml &quot;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&quot; --qc 100 "],["resuming-the-pipeline-command-line-interface.html", "Chapter 8 Resuming the pipeline (Command line interface)", " Chapter 8 Resuming the pipeline (Command line interface) Nextflow provides a powerful command line interface for the execution pipelines. As we have seen before a general way of running Nextflow looks like: nextflow [options] COMMAND [arg...] We often run the workflow using nextflow main_18.nf --inputmzml &quot;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&quot; --qc 100 However, when Nextflow is run it will automatically use its powerful run command. So the above code can actually be run using: nextflow run main_18.nf --inputmzml &quot;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&quot; --qc 100 The run command has several options that can be seen here One the most important option is resume. This option executes the script using the cached results, useful to continue executions that was stopped by an error. So if we imagine the above pipeline fails for example because of an error in one of the processes, after resolving the error, we can run the workflow using: nextflow run main_18.nf -resume --inputmzml &quot;/crex/proj/uppmax2023-2-3/metabolomics/mzMLData/*.mzML&quot; --qc 100 Using this command, the already successfully executed processes will not be rerun. So the pipeline will continue from where it failed last time. You can see the full description of Nextflow CLI here "],["nextflow-assigments.html", "Chapter 9 Nextflow assigments 9.1 Set up the environment 9.2 Part 1 9.3 Part 2", " Chapter 9 Nextflow assigments Here are two assignments for finish the Nextflow part of the course. 9.1 Set up the environment You need to do this every time you start the lap… First ssh to Uppmax ssh -AX youusername@rackham.uppmax.uu.se Make a directory in your userspace mkdir -p /crex/proj/uppmax2023-2-3/nobackup/$USER/nextflow_lab Navigate to the folder you have created cd /crex/proj/uppmax2023-2-3/nobackup/$USER/nextflow_lab Load Nexflow module load bioinfo-tools module load Nextflow/20.10.0 Fix some environmental variables: export NXF_SINGULARITY_CACHEDIR=/crex/proj/uppmax2023-2-3/metabolomics/singularity You are now ready to start! The container for both of the following parts is “metaboigniter/course_docker:v5” and we want to use SLURM to run the jobs. IMPORTANT: DO NOT remove, change, add anything here and in its subfolders: /crex/proj/uppmax2023-2-3/metabolomics For part 1, you should send us the modified main.nf and nextflow.config. For part 2, you will have to send us a main.nf, nextflow.config and a PCA plot! 9.2 Part 1 In this part, we will run a small pipeline together. The pipeline is already in a folder that you need to copy to your personal space. We assume that you are now in nextflow_lab Copy the pipeline and the configuration file cp -r /crex/proj/uppmax2023-2-3/metabolomics/xcms_pipeline . Now you have all the required files in your folder. Try to run the pipeline cd xcms_pipeline nextflow main.nf -profile uppmax --project &quot;uppmax2023-2-3&quot; --clusterOptions &quot;-M snowy&quot; You can open another terminal window and ssh to Uppmax and use jobinfo to see whether your jobs are running or not! If you realized that won’t run for some steps, it might be because of RAM, CPU, or running time. In the nextflow.config, you can change these parameters! Please remember that there is a high chance that Uppmax becomes very slow because the jobs are heavy. So Please consider canceling your jobs if you already know and have a feeling about how Uppmax runs your jobs scancel -u $USER -M snowy Exercise one: There are some hardcoded parameters part of the pipeline: If process process_masstrace_detection_pos_xcms we have the following parameters: peakwidthLow=5 peakwidthHigh=10 noise=1000 polarity=positive Can you change the config file so the value of these parameters is set there and fetched here? How can the user pass these parameters to the pipeline? Show the command! Tip: Your task is to modify the config file (nextflow.config) so that for example peakwidthLow=5 can be replaced by peakwidthLow=$params.peakwidthLow and** the value of peakwidthLow** is set in the config file! Help me: params scope 9.3 Part 2 As talked before the pipeline in the part 1 does mass spectrometry data pre-processing using XCMS software suite. In this part of the exercise, you will need to build a pipeline from scratch that does the same thing using OpenMS. You can of course use the pipeline in part 1 as a template and try to modify it to do that. 9.3.1 Assignment 1 (Build an OpenMS pipeline) This is what you need to do Since the process in this specific pipeline need a separate parameter file you will need three additional file channels The first one is used in the FeatureFinder process and is located here: /crex/proj/uppmax2023-2-3/metabolomics/openms_params/FeatureFinder.ini The second one is for the alignment process and is located here: /crex/proj/uppmax2023-2-3/metabolomics/openms_params/featureAlignment.ini The third one is for the linker process and is located here: /crex/proj/uppmax2023-2-3/metabolomics/openms_params/featureLinker.ini Help me: File channels, How to create file channels Now that we have our channels up and running, time to create four processes: Let’s name the first process process_masstrace_detection_pos_OpenMS This process has two inputs: The first input is from the mzML files The second one is from** the feature detection** parameter file. Please remember that The second input should be a type of input that repeats for each of the mzML file emitted by the first channel The output for this process has the same baseName as input but it has “.featureXML” extension. The name of the output channel must be alignmentProcess And finally, the command needed to run is FeatureFinderMetabo -in inputVariable -out inputBaseName.featureXML -ini settingFIleVariable Please note that you need to change inputVariable, inputBaseName and settingFIleVariable! Please also note that the inputs to this tool are given by -in, the outputs by -out and settings by -ini. Help me: processes, input each (file), bash command The second process is called process_masstrace_alignment_pos_OpenMS Similar to the previous process it will get two inputs: The first one is the output of process_masstrace_detection_pos_OpenMS The second one is from the alignment parameter file. Similar to the previous one this input is also repeating! The output for this process has the same baseName as input but it has .featureXML extension. However, these files are in a different folder (let’s call it out). The output channel must be named LinkerProcess The command needed is a bit different from the previous one: This command accepts ALL the featureXML files at the same time (look at collect!) and outputs the same number of files. The inputs must be separated by space (remember the join operation in the XCMS pipeline?). The output is the same as input however you need to put the output in a different folder within the process! Think about a simple bash script. You create a folder and join the files in a way that they are written in a separate folder! Given three samples, x.featureXML, y.featureXML and z.featureXML, an example of the command is like mkdir out MapAlignerPoseClustering -in x.featureXML y.featureXML z.featureXMLL -out out/x.featureXMLL out/y.featureXML out/z.featureXML -ini setting.ini Please remember the code above is just an example of a command you run in bash You will have to implement this using Nextflow! Please also note that the inputs to this tool are given by -in, the outputs by -out and settings by -ini. Finally, this process needs quite a bit of RAM to go forward. Set the memory for this to probably around 10 GB or even more! Help me: collect, joining in the process!, memory The third process is called process_masstrace_linker_pos_OpenMS and is used to link (merge) multiple files into a single file It will get two inputs: The output from the alignment The parameter file The output of this process is a single file and must be named “Aggregated.consensusXML” it will be sent over to a channel called “textExport” And the command: This is similar to the previous step. You need to gather all the inputs separated by space (collect and join!). However, it will only output one file. Given the same three files above an example of the command would be: FeatureLinkerUnlabeledQT -in x.featureXML y.featureXML z.featureXML -out Aggregated.consensusXML -ini setting.ini Please also note that the inputs to this tool are given by -in, the outputs by -out and settings by -ini. Finally, this process needs quite a bit of RAM to go forward. Set the memory for this to probably around 10 GB or even more! Finally the last step which takes the xml formatted output from the previous step and convert it into a csv file. Let’s call this process process_masstrace_exporter_pos_OpenMS The input to the process is the output from the previous step The output is a single file and must be called Aggregated_clean.csv and sent over to a channel called out To run the command: This process will run two commands the first one does the conversion and the second one the cleaning. We will only need the last output (Aggregated_clean.csv) TextExporter -in input -out Aggregated.csv /usr/bin/readOpenMS.r input=Aggregated.csv output=Aggregated_table.csv Please also note that the inputs to this tool are given by -in, the outputs by -out. Remember that this process should publish its result to a specific directory (your choice). It does that by symlink! Change the mode to copy if you want! Help me: publish the results 9.3.2 Assignment 2 (optional, gives extra points) Given the output from the last step of the pipeline, do a PCA on the data. The output of the last step is a csv file (comma separated) and the first column is the header. The missing values are designated by NA. Can you add your script (PCA) as an extra node part of the pipeline? You don’t have to use containers, you can use conda or whatever you like! There are R and Python already available in the container, you can also use conda or modules If you don’t know what PCA is or how to do it, please let us know! IMPORTANT: some time Uppmax file system might cause some of the processes to fail, if this happens, rerun the workflow with -resume option. This might actually pass the step that was previously shown as failed! If you encounter issues with Uppmax being way too slow or does not do anything, please let up know! "]]
